# -*- coding: utf-8 -*-
"""streaming_infer.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ElbPmqVBGKy07QLYgVFD9ZpwKXJJ8ONU
"""

"""
streaming_infer_file.py
Structured Streaming demo using file source (Windows friendly).
"""

import os
from pyspark.sql import SparkSession
from pyspark.ml import PipelineModel
from pyspark.ml.feature import IndexToString

MODEL_SAVE_PATH = "./saved_pipeline_model"
INPUT_DIR = "./stream_input"   # Create this folder manually

# Create the input directory if it doesn't exist
os.makedirs(INPUT_DIR, exist_ok=True)

spark = SparkSession.builder \
    .appName("NewsSentimentStreamingFile") \
    .getOrCreate()

# Load pipeline model
pipeline_model = PipelineModel.load(MODEL_SAVE_PATH)

# Load labels
labels_path = os.path.join(MODEL_SAVE_PATH, "labels.txt")
with open(labels_path, "r", encoding="utf-8") as f:
    saved_labels = [l.strip() for l in f if l.strip()]

index_to_string = IndexToString(
    inputCol="prediction",
    outputCol="Predicted_Label",
    labels=saved_labels
)

# File source: watch INPUT_DIR for new .txt files
streaming_df = spark.readStream \
    .format("text") \
    .load(INPUT_DIR)

streaming_df = streaming_df.selectExpr("CAST(value AS STRING) as headline")

# Apply model
pred_stream = pipeline_model.transform(streaming_df)
pred_stream = index_to_string.transform(pred_stream)

out = pred_stream.select("headline", "Predicted_Label")

query = out.writeStream \
    .outputMode("append") \
    .format("console") \
    .option("truncate", False) \
    .start()

print(f"Watching folder {INPUT_DIR}. Add .txt files with headlines (one per line).")

query.awaitTermination()