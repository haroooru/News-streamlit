# -*- coding: utf-8 -*-
"""dashboard.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ElbPmqVBGKy07QLYgVFD9ZpwKXJJ8ONU
"""

"""
dashboard.py

Streamlit app that:
- Fetches a batch of latest headlines from the API (or uses fallback),
- Loads the saved PipelineModel,
- Displays predictions (table) and a small bar chart.

Run:
    streamlit run dashboard.py
"""

import os
import requests
import streamlit as st
from pyspark.sql import SparkSession
from pyspark.ml import PipelineModel
from pyspark.ml.feature import IndexToString

# Config
API_KEY = os.environ.get("NEWS_API_KEY", "YOUR_API_KEY_HERE")
NEWS_API_ENDPOINT = f"https://newsdata.io/api/1/news?apikey={API_KEY}&language=en"
MODEL_SAVE_PATH = "./saved_pipeline_model"

st.set_page_config(page_title="News Sentiment Dashboard")

@st.cache_resource
def get_spark():
    return SparkSession.builder.master("local[*]").appName("StreamlitSpark").getOrCreate()

@st.cache_resource
def load_model():
    return PipelineModel.load(MODEL_SAVE_PATH)

def fetch_headlines(limit=20):
    try:
        resp = requests.get(NEWS_API_ENDPOINT, timeout=15)
        resp.raise_for_status()
        data = resp.json()
        results = data.get("results", []) if isinstance(data, dict) else []
        titles = [r.get("title") for r in results if r.get("title")]
        return titles[:limit]
    except Exception as e:
        st.warning(f"Fetch failed: {e}")
        return [
            "Stocks rally after economic boost",
            "Disaster strikes coastal town",
            "Tech companies report record profits",
            "Market crashes amid fears"
        ]

def main():
    st.title("Real-time News Sentiment (Batch Polling Demo)")
    st.write("This dashboard polls an API for the latest headlines and displays predictions.")

    spark = get_spark()
    pipeline_model = load_model()

    # Read labels file
    labels_path = os.path.join(MODEL_SAVE_PATH, "labels.txt")
    if os.path.exists(labels_path):
        with open(labels_path, "r", encoding="utf-8") as f:
            saved_labels = [l.strip() for l in f if l.strip()]
    else:
        saved_labels = None

    st.sidebar.header("Controls")
    limit = st.sidebar.slider("Number of headlines to fetch", 5, 50, 15)
    refresh = st.sidebar.button("Fetch & Predict")

    if refresh:
        headlines = fetch_headlines(limit=limit)
        if not headlines:
            st.error("No headlines fetched.")
            return

        # prepare spark DF
        df = spark.createDataFrame([(h,) for h in headlines], ["headline"])
        preds = pipeline_model.transform(df)

        if saved_labels:
            # convert to string labels
            converter = IndexToString(inputCol="prediction", outputCol="Predicted_Label", labels=saved_labels)
            preds = converter.transform(preds)
            result_df = preds.select("headline", "Predicted_Label", "probability").toPandas()
            # keep probability as string for nicer display
            result_df["prob"] = result_df["probability"].apply(lambda p: float(max(p)) if p is not None else None)
            result_df = result_df.drop(columns=["probability"])
            st.subheader("Predictions")
            st.dataframe(result_df)
            # Bar chart
            counts = result_df["Predicted_Label"].value_counts().rename_axis('label').reset_index(name='counts')
            counts = counts.set_index('label')
            st.bar_chart(counts)
        else:
            st.error("Labels file not found. Predictions will show numeric indices.")
            st.dataframe(preds.select("headline", "prediction").toPandas())

    st.markdown("---")
    st.write("Notes: This is a demo polling approach. For true real-time streaming, use a Kafka source or socket + Structured Streaming.")

if __name__ == "__main__":
    main()